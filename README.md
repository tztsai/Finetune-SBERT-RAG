# Finetune-SBERT-RAG

This repository contains the implementation of fine-tuning SBERT (Sentence-BERT) for a Retrieval-Augmented Generation (RAG) system. The main goal of the project is to improve the performance of a RAG pipeline by enhancing the quality of the embeddings generated by SBERT. RAG is a powerful approach that combines retrieval of relevant documents with generation models like GPT, enabling the system to generate informed, context-aware responses.

## Table of Contents
- [Introduction](#introduction)
- [Requirements](#requirements)
- [Project Structure](#project-structure)
- [Usage](#usage)
- [Dataset](#dataset)
- [Training](#training)
- [Evaluation](#evaluation)
- [Acknowledgements](#acknowledgements)

## Introduction

RAG leverages a two-step process for generating answers to queries:
1. **Retrieval**: Using a retriever model to fetch the most relevant documents.
2. **Generation**: Utilizing a generative model to produce a coherent and informative response based on the retrieved documents.

This project fine-tunes SBERT, a state-of-the-art sentence embedding model, to improve the quality of document retrieval, which in turn enhances the quality of the generated answers in a RAG system.

## Requirements

To run this project, you will need the following dependencies:
- Python 3.7+
- PyTorch
- Hugging Face Transformers
- Sentence-Transformers
- FAISS (for efficient similarity search)
- Other necessary packages are listed in `requirements.txt`.

Install the dependencies using:

```bash
pip install -r requirements.txt
```

## Project Structure

- `main.ipynb`: The main notebook that contains the implementation for loading data, fine-tuning SBERT, and evaluating the retrieval performance.
- `src/`: Contains helper scripts for data processing, model evaluation, and utility functions.
- `data/`: Placeholder for datasets used during training and evaluation.

## Usage

### 1. Clone the repository
```bash
git clone https://github.com/tztsai/Finetune-SBERT-RAG.git
cd Finetune-SBERT-RAG
```

### 2. Data Preparation

Ensure that you have a dataset suitable for fine-tuning SBERT in the context of document retrieval. The dataset should have pairs of queries and relevant documents.

Example format:
- `queries.txt`: Contains a list of queries.
- `documents.txt`: Contains corresponding relevant documents.

You can modify the dataset loading and pre-processing steps in the `main.ipynb` file to fit your specific dataset.

### 3. Fine-tuning SBERT

Open `main.ipynb` and follow the steps to load your dataset and fine-tune the SBERT model. The notebook includes the following steps:
- Load pre-trained SBERT.
- Prepare dataset for training and validation.
- Fine-tune SBERT using the contrastive learning objective.
- Save the fine-tuned model.

### 4. Evaluating Retrieval Performance

After fine-tuning, evaluate the performance of the retriever model by running retrieval tasks on a test set. The retrieval performance can be measured using metrics such as precision, recall, and mean reciprocal rank (MRR).

## Dataset

The dataset required for this project consists of pairs of queries and relevant documents. You can use publicly available datasets like:
- **Natural Questions (NQ)**
- **MS MARCO**

Make sure the dataset is in a format compatible with SBERT fine-tuning.

## Training

Once the dataset is prepared and preprocessed, follow the steps in the notebook to start training. You can adjust hyperparameters like learning rate, batch size, and number of epochs directly in the notebook.

To run the fine-tuning process:

1. Load the dataset.
2. Train the SBERT model on your query-document pairs.
3. Monitor the validation performance during training.

The fine-tuned model will be saved for later use in a RAG pipeline.

## Evaluation

After fine-tuning, you can evaluate the model by retrieving relevant documents for a set of queries and comparing the results with the ground truth. The notebook includes code for evaluating the model using common retrieval metrics.

## Acknowledgements

This project is built on top of:
- [Sentence-BERT](https://www.sbert.net/)
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [FAISS](https://github.com/facebookresearch/faiss)
